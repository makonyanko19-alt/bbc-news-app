import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
from googletrans import Translator
import time

# --- ç”»é¢ã®ãƒ‡ã‚¶ã‚¤ãƒ³ ---
st.title("ğŸŒ Global News Collector")
st.write("BBC Business Newsã‚’å–å¾—ã—ã€æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦è¡¨ç¤ºã—ã¾ã™ã€‚")

# ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸã‚‰å®Ÿè¡Œã™ã‚‹
if st.button("ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã™ã‚‹"):
    
    # é€²è¡ŒçŠ¶æ³ãƒãƒ¼ã‚’è¡¨ç¤º
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    url = "https://www.bbc.com/news/business"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    translator = Translator()
    
    status_text.text("BBCã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, "html.parser")
    headlines = soup.find_all("h2")
    
    news_list = []
    total_headlines = len(headlines)

    # ãƒ«ãƒ¼ãƒ—å‡¦ç†
    for i, headline in enumerate(headlines):
        # é€²æ—ãƒãƒ¼ã‚’æ›´æ–°
        progress = (i + 1) / total_headlines
        progress_bar.progress(progress)
        status_text.text(f"åé›†ä¸­... ({i+1}/{total_headlines})")

        h2_text = headline.text.strip()
        if not h2_text:
            continue

        link_tag = headline.find("a")
        if not link_tag:
            link_tag = headline.find_parent("a")

        full_url = ""
        link_text = ""
        if link_tag:
            link_url = link_tag.get("href")
            if not link_url.startswith("http"):
                full_url = "https://www.bbc.com" + link_url
            else:
                full_url = link_url
            link_text = link_tag.text.strip()

        final_title = h2_text
        if h2_text in ["Business Daily", "Latest audio", "Must watch"]:
            if link_text and link_text != h2_text:
                final_title = f"[Audio] {link_text}"
            else:
                sibling_p = headline.find_next_sibling("p")
                if sibling_p:
                     final_title = f"[Audio] {sibling_p.text.strip()}"

        # ç¿»è¨³
        try:
            translated = translator.translate(final_title, src='en', dest='ja')
            ja_title = translated.text
        except:
            ja_title = "(ç¿»è¨³å¤±æ•—)"
        
        news_data = {
            "ã‚¿ã‚¤ãƒˆãƒ« (æ—¥æœ¬èª)": ja_title,
            "Title (English)": final_title,
            "URL": full_url
        }
        news_list.append(news_data)
        time.sleep(1) # ç¿»è¨³ã‚µãƒ¼ãƒãƒ¼ã¸ã®é…æ…®

    # å…¨éƒ¨çµ‚ã‚ã£ãŸã‚‰
    status_text.text("å®Œäº†ï¼")
    progress_bar.empty()

    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    df = pd.DataFrame(news_list)

    # ç”»é¢ã«ãƒ‰ãƒ¼ãƒ³ï¼ã¨è¡¨ã‚’è¡¨ç¤º (ã“ã“ã‚’ä¿®æ­£ã—ã¾ã—ãŸï¼)
    st.success(f"{len(df)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã—ã¾ã—ãŸï¼")
    
    st.dataframe(
        df,
        column_config={
            "URL": st.column_config.LinkColumn(
                "è¨˜äº‹ãƒªãƒ³ã‚¯",           # åˆ—ã®åå‰
                display_text="Link"    # URLã®ä»£ã‚ã‚Šã«è¡¨ç¤ºã™ã‚‹æ–‡å­—ï¼ˆã“ã‚ŒãŒãªã„ã¨é•·ã„URLãŒè¡¨ç¤ºã•ã‚Œã¾ã™ï¼‰
            )
        },
        hide_index=True # å·¦ç«¯ã® 0, 1, 2... ã¨ã„ã†æ•°å­—ã‚’éš ã—ã¦ã‚¹ãƒƒã‚­ãƒªã•ã›ã‚‹
    )

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã‚’è¨­ç½® (CSVä¿å­˜æ©Ÿèƒ½)
    csv = df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
    st.download_button(
        label="CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv,
        file_name='bbc_news_web.csv',
        mime='text/csv',
    )